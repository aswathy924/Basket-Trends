{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVDy_DPfFUIg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctv1GkXRFZ4j",
        "outputId": "56f4a324-eff8-4b86-d06a-9e8b537bec43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2o4fM5DLoDPE9ytrEnzf5GvP8Yc_2cNnSGLck7F6TE836DapK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwlPUBKMTUyr",
        "outputId": "6c9e1e6b-ef11-4e99-95e3-d1e95ee59383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.39.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNmfQbwBO740",
        "outputId": "b600b1f6-5bd2-4016-c8ee-789eb94d6f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from itertools import combinations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "\n",
        "class MarketBasketAnalysis:\n",
        "    def __init__(self, data, min_support=0.05, min_confidence=0.5):\n",
        "        self.data = data\n",
        "        self.min_support = min_support\n",
        "        self.min_confidence = min_confidence\n",
        "        self.transactions = []\n",
        "        self.frequent_itemsets = {}\n",
        "        self.rules = []\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Create transactions from the loaded dataset.\"\"\"\n",
        "        df = self.data\n",
        "        st.write(\"Data Loaded:\")\n",
        "\n",
        "        # Group items by 'Member_number' to create transactions\n",
        "        self.transactions = df.groupby('Member_number')['itemDescription'].apply(list).reset_index()['itemDescription'].tolist()\n",
        "\n",
        "        # Display total number of transactions and unique items\n",
        "        total_transactions = len(self.transactions)\n",
        "        unique_items = df['itemDescription'].nunique()\n",
        "\n",
        "        st.write(f\"Total Transactions: {total_transactions}\")\n",
        "        st.write(f\"Total Unique Items: {unique_items}\")\n",
        "\n",
        "    def apriori(self):\n",
        "        \"\"\"Run the Apriori algorithm to find frequent itemsets and generate rules.\"\"\"\n",
        "        item_count = {}\n",
        "        num_transactions = len(self.transactions)\n",
        "\n",
        "        # Count item frequency\n",
        "        for transaction in self.transactions:\n",
        "            for item in transaction:\n",
        "                item_count[item] = item_count.get(item, 0) + 1\n",
        "\n",
        "        # Create initial frequent itemsets (1-itemsets)\n",
        "        for item, count in item_count.items():\n",
        "            if count / num_transactions >= self.min_support:\n",
        "                self.frequent_itemsets[frozenset([item])] = count\n",
        "\n",
        "        k = 2\n",
        "\n",
        "        # Generate frequent itemsets\n",
        "        while True:\n",
        "            candidate_itemsets = {}\n",
        "            frequent_items = list(self.frequent_itemsets.keys())\n",
        "\n",
        "            # Generate new candidates from previous frequent itemsets\n",
        "            for i in range(len(frequent_items)):\n",
        "                for j in range(i + 1, len(frequent_items)):\n",
        "                    candidate = frequent_items[i] | frequent_items[j]\n",
        "                    if len(candidate) == k:  # Only include k-size itemsets\n",
        "                        candidate_itemsets[candidate] = 0\n",
        "\n",
        "            # Count support for candidate itemsets\n",
        "            for transaction in self.transactions:\n",
        "                transaction_set = frozenset(transaction)\n",
        "                for candidate in candidate_itemsets:\n",
        "                    if candidate.issubset(transaction_set):\n",
        "                        candidate_itemsets[candidate] += 1\n",
        "\n",
        "            # Prune candidates that do not meet min_support\n",
        "            frequent_itemsets = {}\n",
        "            for candidate, count in candidate_itemsets.items():\n",
        "                if count / num_transactions >= self.min_support:\n",
        "                    frequent_itemsets[candidate] = count\n",
        "                    self.frequent_itemsets[candidate] = count\n",
        "\n",
        "            if not frequent_itemsets:\n",
        "                break\n",
        "\n",
        "            k += 1\n",
        "\n",
        "        self.generate_rules()\n",
        "\n",
        "    def generate_rules(self):\n",
        "        \"\"\"Generate association rules from frequent itemsets.\"\"\"\n",
        "        for itemset in self.frequent_itemsets:\n",
        "            if len(itemset) > 1:\n",
        "                for i in range(1, len(itemset)):\n",
        "                    for antecedent in combinations(itemset, i):\n",
        "                        antecedent = frozenset(antecedent)\n",
        "                        consequent = itemset - antecedent\n",
        "                        if consequent:\n",
        "                            support = self.frequent_itemsets[itemset] / len(self.transactions)\n",
        "                            confidence = self.frequent_itemsets[itemset] / self.frequent_itemsets.get(antecedent, 1)\n",
        "                            consequent_support = self.frequent_itemsets.get(consequent, 1) / len(self.transactions)\n",
        "                            lift = confidence / consequent_support if consequent_support > 0 else 0\n",
        "\n",
        "                            if confidence >= self.min_confidence:\n",
        "                                self.rules.append((antecedent, consequent, support, confidence, lift))\n",
        "\n",
        "class MarketBasketVisualization:\n",
        "    def __init__(self, mba):\n",
        "        self.mba = mba\n",
        "\n",
        "    def plot_frequent_itemsets(self, top_n=10):\n",
        "        \"\"\"Plot the top N frequent itemsets.\"\"\"\n",
        "        sorted_itemsets = sorted(self.mba.frequent_itemsets.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "        itemsets, counts = zip(*sorted_itemsets)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.bar([str(set(itemset)) for itemset in itemsets], counts, color='skyblue')\n",
        "        ax.set_xlabel('Itemsets')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        ax.set_title(f'Top {top_n} Frequent Itemsets')\n",
        "        ax.set_xticklabels([str(set(itemset)) for itemset in itemsets], rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)  # Display plot in Streamlit\n",
        "\n",
        "    def plot_support_vs_confidence(self):\n",
        "        \"\"\"Plot support vs confidence for the generated rules.\"\"\"\n",
        "        supports = [rule[2] for rule in self.mba.rules]\n",
        "        confidences = [rule[3] for rule in self.mba.rules]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.scatter(supports, confidences, alpha=0.7)\n",
        "        ax.set_title('Support vs Confidence of Rules')\n",
        "        ax.set_xlabel('Support')\n",
        "        ax.set_ylabel('Confidence')\n",
        "        ax.axhline(y=self.mba.min_confidence, color='r', linestyle='--')\n",
        "        ax.axvline(x=self.mba.min_support, color='g', linestyle='--')\n",
        "        ax.grid()\n",
        "        st.pyplot(fig)  # Display plot in Streamlit\n",
        "\n",
        "    def plot_lift_distribution(self):\n",
        "        \"\"\"Plot the distribution of lift values.\"\"\"\n",
        "        lifts = [rule[4] for rule in self.mba.rules]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.hist(lifts, bins=20, color='lightcoral', alpha=0.7)\n",
        "        ax.set_xlabel('Lift')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        ax.set_title('Distribution of Lift Values')\n",
        "        ax.grid()\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)  # Display plot in Streamlit\n",
        "\n",
        "    def plot_heatmap_of_rules(self):\n",
        "        \"\"\"Plot a heatmap of association rules.\"\"\"\n",
        "        rule_df = pd.DataFrame(self.mba.rules, columns=['Antecedent', 'Consequent', 'Support', 'Confidence', 'Lift'])\n",
        "\n",
        "        # Format antecedents and consequents for display\n",
        "        rule_df['Antecedent'] = rule_df['Antecedent'].apply(lambda x: ', '.join(list(x)))\n",
        "        rule_df['Consequent'] = rule_df['Consequent'].apply(lambda x: ', '.join(list(x)))\n",
        "\n",
        "        # Create a pivot table for the heatmap\n",
        "        pivot_table = rule_df.pivot(index=\"Antecedent\", columns=\"Consequent\", values=\"Confidence\")\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={'label': 'Confidence'}, ax=ax)\n",
        "        ax.set_title('Heatmap of Association Rules')\n",
        "        ax.set_xlabel('Consequent')\n",
        "        ax.set_ylabel('Antecedent')\n",
        "        st.pyplot(fig)  # Display plot in Streamlit\n",
        "\n",
        "    def plot_network_graph(self):\n",
        "        \"\"\"Plot a network graph of association rules.\"\"\"\n",
        "        G = nx.Graph()  # Create an undirected graph for better visualization\n",
        "\n",
        "        # Add edges to the graph\n",
        "        for antecedent, consequent, support, confidence, lift in self.mba.rules:\n",
        "            for a in antecedent:\n",
        "                for c in consequent:\n",
        "                    G.add_edge(str(a), str(c), weight=support)\n",
        "\n",
        "        # Create a layout for the nodes\n",
        "        pos = nx.spring_layout(G, k=0.5)  # You can adjust k for spacing\n",
        "\n",
        "        # Create the figure and axis\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "        # Draw nodes and edges\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=700, node_color='lightblue', ax=ax)\n",
        "        nx.draw_networkx_edges(G, pos, arrowstyle='-|>', arrowsize=20, edge_color='gray', alpha=0.5, ax=ax)\n",
        "        nx.draw_networkx_labels(G, pos, font_size=12, ax=ax)\n",
        "\n",
        "        # Set title and hide axes\n",
        "        ax.set_title('Network Graph of Association Rules')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Display the plot in Streamlit\n",
        "        st.pyplot(fig)\n",
        "\n",
        "def main():\n",
        "    st.title(\"Market Basket Analysis\")\n",
        "\n",
        "    # File uploader for the dataset\n",
        "    uploaded_file = st.file_uploader(\"Upload Groceries Dataset\", type=[\"csv\"])\n",
        "    if uploaded_file is not None:\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "        st.dataframe(df.head())  # Display the first few rows of the dataset\n",
        "\n",
        "        # Create an instance of MarketBasketAnalysis\n",
        "        mba = MarketBasketAnalysis(data=df)\n",
        "        mba.load_data()  # Load the data and create transactions\n",
        "        mba.apriori()  # Run the Apriori algorithm\n",
        "\n",
        "        # Display Frequent Itemsets\n",
        "        st.subheader(\"Frequent Itemsets:\")\n",
        "        itemsets_df = pd.DataFrame(list(mba.frequent_itemsets.items()), columns=['Itemset', 'Count'])\n",
        "        itemsets_df['Itemset'] = itemsets_df['Itemset'].apply(lambda x: ', '.join(list(x)))\n",
        "        st.dataframe(itemsets_df, height=400, use_container_width=True)\n",
        "\n",
        "        # Display Association Rules\n",
        "        st.subheader(\"Association Rules:\")\n",
        "        rules_df = pd.DataFrame(mba.rules, columns=['Antecedent', 'Consequent', 'Support', 'Confidence', 'Lift'])\n",
        "\n",
        "        # Format antecedents and consequents for display\n",
        "        rules_df['Antecedent'] = rules_df['Antecedent'].apply(lambda x: ', '.join(list(x)))\n",
        "        rules_df['Consequent'] = rules_df['Consequent'].apply(lambda x: ', '.join(list(x)))\n",
        "        st.dataframe(rules_df)\n",
        "\n",
        "        # Create an instance of MarketBasketVisualization\n",
        "        mbv = MarketBasketVisualization(mba)\n",
        "\n",
        "        # Visualizations\n",
        "        st.subheader(\"Visualizations\")\n",
        "        visualization_option = st.selectbox(\"Select Visualization\",\n",
        "                                              [\"Top Frequent Itemsets\",\n",
        "                                               \"Support vs Confidence\",\n",
        "                                               \"Lift Distribution\",\n",
        "                                               \"Heatmap of Association Rules\",\n",
        "                                               \"Network Graph of Association Rules\"])\n",
        "\n",
        "        if visualization_option == \"Top Frequent Itemsets\":\n",
        "            mbv.plot_frequent_itemsets(top_n=10)\n",
        "        elif visualization_option == \"Support vs Confidence\":\n",
        "            mbv.plot_support_vs_confidence()\n",
        "        elif visualization_option == \"Lift Distribution\":\n",
        "            mbv.plot_lift_distribution()\n",
        "        elif visualization_option == \"Heatmap of Association Rules\":\n",
        "            mbv.plot_heatmap_of_rules()\n",
        "        elif visualization_option == \"Network Graph of Association Rules\":\n",
        "            mbv.plot_network_graph()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-iaN_q9ifCL",
        "outputId": "480d7955-b107-47ef-ac48-9a408bde857f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is running on: NgrokTunnel: \"https://0736-34-48-2-149.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-10-29T08:51:08+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import os\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Function to run Streamlit\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py\")\n",
        "\n",
        "# Start Streamlit in a background thread\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "# Allow time for Streamlit to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Expose the Streamlit app on a public URL using ngrok\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Streamlit app is running on: {public_url}\")\n",
        "\n",
        "    # Keep the script running to maintain the ngrok tunnel\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error starting ngrok: {e}\")\n",
        "finally:\n",
        "    print(\"Shutting down...\")\n",
        "    ngrok.disconnect(public_url)  # Disconnect ngrok if it was connected\n",
        "    thread.join()  # Wait for the Streamlit thread to finish\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}